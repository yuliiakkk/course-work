import os
import cv2
import torch
import numpy as np
from PIL import Image
from model import UNet
from lama.saicinpainting.training.trainers import load_checkpoint
from lama.saicinpainting.evaluation.utils import move_to_device
from lama.saicinpainting.evaluation.data import pad_img_to_modulo
from transformers import CLIPSegProcessor, CLIPSegForImageSegmentation
from omegaconf import OmegaConf

# üìÅ –®–ª—è—Ö–∏
IMAGE_DIR = "dataset/images"
RESULTS_DIR = "results"
MODEL_PATH = "model/final_model.pth"
LAMA_MODEL_DIR = "lama/big-lama"

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è UNet
if not os.path.exists(MODEL_PATH):
    raise FileNotFoundError("‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ model/final_model.pth ‚Äî —Å–ø–æ—á–∞—Ç–∫—É –Ω–∞—Ç—Ä–µ–Ω—É–π—Ç–µ –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ train.py")

unet = UNet(in_channels=4, out_channels=1).to(DEVICE)
unet.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
unet.eval()

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è CLIPSeg
clipseg_model = CLIPSegForImageSegmentation.from_pretrained("CIDAS/clipseg-rd64-refined").to(DEVICE)
clipseg_processor = CLIPSegProcessor.from_pretrained("CIDAS/clipseg-rd64-refined")

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è LaMa
lama_cfg = OmegaConf.load(os.path.join(LAMA_MODEL_DIR, "config.yaml"))
lama_model = load_checkpoint(path=os.path.join(LAMA_MODEL_DIR, "models", "best.ckpt"),
                             strict=False, map_location="cpu")
lama_model.freeze()
lama_model.to(DEVICE)

# üî§ –í–≤—ñ–¥ –æ–±'—î–∫—Ç–∞
prompt = input("–í–≤–µ–¥–∏ –Ω–∞–∑–≤—É –æ–±'—î–∫—Ç–∞ –¥–ª—è –≤–∏–¥–∞–ª–µ–Ω–Ω—è: ").strip()
os.makedirs(RESULTS_DIR, exist_ok=True)

# üìÇ –û–±—Ö—ñ–¥ —É—Å—ñ—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å
for filename in sorted(os.listdir(IMAGE_DIR)):
    if not filename.endswith(".jpg"):
        continue

    image_path = os.path.join(IMAGE_DIR, filename)
    print(f"üîç –û–±—Ä–æ–±–∫–∞: {filename}")

    image_pil = Image.open(image_path).convert("RGB")
    image_cv = cv2.imread(image_path)

    # CLIPSeg
    inputs = clipseg_processor(text=prompt, images=image_pil, return_tensors="pt").to(DEVICE)
    with torch.no_grad():
        outputs = clipseg_model(**inputs)
    clipseg_mask = outputs.logits[0][0].cpu().numpy()
    clipseg_mask = (clipseg_mask > 0.3).astype(np.uint8) * 255

    # Resize CLIPSeg –º–∞—Å–∫–∏
    clipseg_mask_resized = cv2.resize(clipseg_mask, image_pil.size, interpolation=cv2.INTER_NEAREST)

    # –û–±'—î–¥–Ω–∞–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è + –º–∞—Å–∫–∏ ‚Üí UNet
    image_tensor = torch.from_numpy(image_cv[:, :, ::-1].copy()).permute(2, 0, 1).float() / 255.0
    clipseg_tensor = torch.from_numpy(clipseg_mask_resized).unsqueeze(0).float() / 255.0
    input_tensor = torch.cat([image_tensor, clipseg_tensor], dim=0).unsqueeze(0).to(DEVICE)

    # UNet
    with torch.no_grad():
        predicted_mask = unet(input_tensor)[0][0].cpu().numpy()
    predicted_mask_bin = (predicted_mask > 0.5).astype(np.uint8) * 255

    # LaMa
    batch = {
        "image": cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB).astype("uint8"),
        "mask": predicted_mask_bin.astype("uint8")
    }
    batch = pad_img_to_modulo(batch, 8)
    batch = move_to_device(batch, DEVICE)

    with torch.no_grad():
        result = lama_model(batch)[0].permute(1, 2, 0).cpu().numpy().astype("uint8")

    # üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è
    result_bgr = cv2.cvtColor(result, cv2.COLOR_RGB2BGR)
    result_path = os.path.join(RESULTS_DIR, filename)
    cv2.imwrite(result_path, result_bgr)
    print(f"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {result_path}")